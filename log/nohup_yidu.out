Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
﻿，患者3个月前（2014年2月）因腹胀进行性加重就诊于我院，，行胃镜检查示：贲门低分化腺癌。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O']
遂于我院胃肠外科行根治性全胃切除术。术中见肿物位于贲门小弯侧，侵及浆膜。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O']
，术后病理：(胃)低分化腺癌，(淋巴结)淋巴结转移癌(3/14)，另见淋巴结旁癌结节一枚，，病理号：Z0470555。 ['O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
，术后分期：T3N2M0OIIIA期。术后腹痛消失，胃区偶有牵拉样疼痛。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
体能恢复良好，为进一步治疗入我科。完善相关检查，患者肾功能略有异常，为减少肾功能损伤，予SOX方案化疗2周期，，用药为：奥沙利铂200毫克D1静点，替吉奥120毫克日二次口服D1-7。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG', 'I-DRUG', 'I-DRUG', 'I-DRUG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DRUG', 'I-DRUG', 'I-DRUG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
末次化疗前7日患者出现I°恶心、呕吐，7日后逐渐好转。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
化疗后未出现骨髓抑制，现患者化疗结束3周，为求再次化疗入院。 ['O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
患者近2周来饮食睡眠一般，精神体力可，二便正常，病来体重上降15公斤，ECOG评分1分。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
，患者2月前因“进食梗阻感半月余”于2015.12.29在全麻上行近端胃癌根治术，手术顺利，术后患者恢复良好，术后常规病理示（胃）低-中分化腺癌伴神经内分泌分化，溃疡型，肿瘤切面积3*0.8CM，侵达浆膜。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'I-OPERATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
下、上切缘及吻合器切缘未查见癌。小弯侧淋巴结12枚，大弯侧淋巴结1枚及另送食管旁淋巴结1枚，均未查见转移癌。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
﻿入院前7年无明显诱因患者感活动后乏力、气促，不伴头昏、头痛、胸闷、心悸、胸痛、晕厥、黑曚、呃逆、反酸、嗳气、烧心、恶心、呕吐、腹痛、腹胀、腹泻、黑便。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'O']
患者于*****医院诊断“溶血性贫血”，给予输血等处理后病情缓解。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
病程中如下症状反复发生，多次给予输血治疗后病程好转。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
2013年4月*****胸片检查：两肺纹理增多，心影轻度增大。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTIMAGE', 'I-TESTIMAGE', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O']
心脏彩超：右房、右室稍增大，EF正常。血库：间接Coomb’s试验（+）P，阳性对照（+++），P。 ['B-TESTIMAGE', 'I-TESTIMAGE', 'I-TESTIMAGE', 'I-TESTIMAGE', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
骨髓象：增生性骨髓象，红系增生旺盛。2014-4-24（******医院）分钟遗传学检验报告单：未检测到送检样本中G6PD基因G1388A，G1376T，A95G三个常见突变位点，结合临床；新生儿疾病筛查报告单：新生儿G6PD活性试验1.56（正常）。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
肝功：总胆红素O50umol/L，直接胆红素O10umol/L，ALTO40U/L，ASTO18U/L。 ['O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TESTLAB', 'I-TESTLAB', 'I-TESTLAB', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
诊断“G-6-PD酶缺乏症，贫血性心脏病，肝功能不全”明确。 ['O', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O']
经输洗涤红细胞后病情好转。出院后患者反复出现乏力、气促不适，反复经输注洗涤红细胞后缓解。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
7天前可疑受凉后出现血尿，患者未予处理。今再次出现四肢乏力，活动后气促伴耳鸣。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'O', 'O']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
load bert Model start!
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.
load bert Model end!
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
Input-Token (InputLayer)        (None, 256)          0
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, 256)          0
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, 256, 768), ( 16226304    Input-Token[0][0]
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, 256, 768)     1536        Input-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, 256, 768)     0           Embedding-Token[0][0]
                                                                 Embedding-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, 256, 768)     196608      Embedding-Token-Segment[0][0]
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, 256, 768)     0           Embedding-Position[0][0]
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, 256, 768)     1536        Embedding-Dropout[0][0]
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     2362368     Embedding-Norm[0][0]
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     0           Embedding-Norm[0][0]
                                                                 Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-1-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-1-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-1-MultiHeadSelfAttention-
                                                                 Encoder-1-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-1-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-1-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-1-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-1-FeedForward-Norm[0][0]
                                                                 Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-2-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-2-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-2-MultiHeadSelfAttention-
                                                                 Encoder-2-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-2-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-2-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-2-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-2-FeedForward-Norm[0][0]
                                                                 Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-3-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-3-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-3-MultiHeadSelfAttention-
                                                                 Encoder-3-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-3-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-3-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-3-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-3-FeedForward-Norm[0][0]
                                                                 Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-4-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-4-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-4-MultiHeadSelfAttention-
                                                                 Encoder-4-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-4-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-4-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-4-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-4-FeedForward-Norm[0][0]
                                                                 Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-5-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-5-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-5-MultiHeadSelfAttention-
                                                                 Encoder-5-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-5-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-5-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-5-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-5-FeedForward-Norm[0][0]
                                                                 Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-6-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-6-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-6-MultiHeadSelfAttention-
                                                                 Encoder-6-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-6-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-6-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-6-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-6-FeedForward-Norm[0][0]
                                                                 Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-7-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-7-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-7-MultiHeadSelfAttention-
                                                                 Encoder-7-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-7-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-7-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-7-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-7-FeedForward-Norm[0][0]
                                                                 Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-8-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-8-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-8-MultiHeadSelfAttention-
                                                                 Encoder-8-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-8-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-8-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     2362368     Encoder-8-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     0           Encoder-8-FeedForward-Norm[0][0]
                                                                 Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 256, 768)     1536        Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-FeedForward (FeedForw (None, 256, 768)     4722432     Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-FeedForward-Dropout ( (None, 256, 768)     0           Encoder-9-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-9-FeedForward-Add (Add) (None, 256, 768)     0           Encoder-9-MultiHeadSelfAttention-
                                                                 Encoder-9-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-9-FeedForward-Norm (Lay (None, 256, 768)     1536        Encoder-9-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-9-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-9-FeedForward-Norm[0][0]
                                                                 Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-FeedForward-Dropout  (None, 256, 768)     0           Encoder-10-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-10-FeedForward-Add (Add (None, 256, 768)     0           Encoder-10-MultiHeadSelfAttention
                                                                 Encoder-10-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-10-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-10-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-10-FeedForward-Norm[0][0]
                                                                 Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-FeedForward-Dropout  (None, 256, 768)     0           Encoder-11-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-11-FeedForward-Add (Add (None, 256, 768)     0           Encoder-11-MultiHeadSelfAttention
                                                                 Encoder-11-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-11-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-11-FeedForward-Add[0][0]
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     0           Encoder-11-FeedForward-Norm[0][0]
                                                                 Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 256, 768)     1536        Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-FeedForward (FeedFor (None, 256, 768)     4722432     Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-FeedForward-Dropout  (None, 256, 768)     0           Encoder-12-FeedForward[0][0]
__________________________________________________________________________________________________
Encoder-12-FeedForward-Add (Add (None, 256, 768)     0           Encoder-12-MultiHeadSelfAttention
                                                                 Encoder-12-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-12-FeedForward-Norm (La (None, 256, 768)     1536        Encoder-12-FeedForward-Add[0][0]
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 256, 128)     426496      Encoder-12-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 256, 13)      1677        bidirectional_1[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 256, 13)      0           time_distributed_1[0][0]
__________________________________________________________________________________________________
crf_1 (CRF)                     (None, 256, 13)      377         dropout_1[0][0]
==================================================================================================
Total params: 101,908,998
Trainable params: 101,908,998
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
Train on 4948 samples, validate on 653 samples
Epoch 1/200
4948/4948 [==============================] - 812s 164ms/step - loss: 12.4271 - crf_accuracy: 0.4796 - val_loss: 11.7064 - val_crf_accuracy: 0.8143
Epoch 00001: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep01.h5
Epoch 2/200
4948/4948 [==============================] - 778s 157ms/step - loss: 11.4784 - crf_accuracy: 0.8044 - val_loss: 11.3742 - val_crf_accuracy: 0.8255
Epoch 00002: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep02.h5
Epoch 3/200
4948/4948 [==============================] - 793s 160ms/step - loss: 11.1691 - crf_accuracy: 0.8325 - val_loss: 11.1563 - val_crf_accuracy: 0.8914
Epoch 00003: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep03.h5
Epoch 4/200
4948/4948 [==============================] - 773s 156ms/step - loss: 10.9845 - crf_accuracy: 0.8792 - val_loss: 11.0668 - val_crf_accuracy: 0.9144
Epoch 00004: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep04.h5
Epoch 5/200
4948/4948 [==============================] - 764s 154ms/step - loss: 10.8932 - crf_accuracy: 0.9024 - val_loss: 11.0174 - val_crf_accuracy: 0.9340
Epoch 00005: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep05.h5
Epoch 6/200
4948/4948 [==============================] - 770s 156ms/step - loss: 10.8285 - crf_accuracy: 0.9230 - val_loss: 10.9717 - val_crf_accuracy: 0.9415
Epoch 00006: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep06.h5
Epoch 7/200
4948/4948 [==============================] - 773s 156ms/step - loss: 10.7843 - crf_accuracy: 0.9362 - val_loss: 10.9648 - val_crf_accuracy: 0.9471
Epoch 00007: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep07.h5
Epoch 8/200
4948/4948 [==============================] - 780s 158ms/step - loss: 10.7493 - crf_accuracy: 0.9454 - val_loss: 10.9657 - val_crf_accuracy: 0.9504
Epoch 00008: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep08.h5
Epoch 9/200
4948/4948 [==============================] - 783s 158ms/step - loss: 10.7219 - crf_accuracy: 0.9526 - val_loss: 10.9424 - val_crf_accuracy: 0.9506
Epoch 00009: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep09.h5
Epoch 10/200
4948/4948 [==============================] - 769s 155ms/step - loss: 10.7014 - crf_accuracy: 0.9576 - val_loss: 10.9420 - val_crf_accuracy: 0.9526
Epoch 00010: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep10.h5
Epoch 11/200
4948/4948 [==============================] - 761s 154ms/step - loss: 10.6795 - crf_accuracy: 0.9627 - val_loss: 10.9402 - val_crf_accuracy: 0.9528
Epoch 00011: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep11.h5
Epoch 12/200
4948/4948 [==============================] - 782s 158ms/step - loss: 10.6626 - crf_accuracy: 0.9665 - val_loss: 10.9417 - val_crf_accuracy: 0.9524
Epoch 00012: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep12.h5
Epoch 13/200
4948/4948 [==============================] - 784s 158ms/step - loss: 10.6564 - crf_accuracy: 0.9678 - val_loss: 10.9421 - val_crf_accuracy: 0.9503
Epoch 00013: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep13.h5
Epoch 14/200
4948/4948 [==============================] - 785s 159ms/step - loss: 10.6385 - crf_accuracy: 0.9719 - val_loss: 10.9581 - val_crf_accuracy: 0.9522
Epoch 00014: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep14.h5
Epoch 15/200
4948/4948 [==============================] - 770s 156ms/step - loss: 10.6348 - crf_accuracy: 0.9726 - val_loss: 10.9673 - val_crf_accuracy: 0.9507
Epoch 00015: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep15.h5
Epoch 16/200
4948/4948 [==============================] - 769s 155ms/step - loss: 10.6184 - crf_accuracy: 0.9768 - val_loss: 10.9694 - val_crf_accuracy: 0.9533
Epoch 00016: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep16.h5
Epoch 17/200
4948/4948 [==============================] - 773s 156ms/step - loss: 10.6125 - crf_accuracy: 0.9771 - val_loss: 10.9635 - val_crf_accuracy: 0.9527
Epoch 00017: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep17.h5
Epoch 18/200
4948/4948 [==============================] - 771s 156ms/step - loss: 10.6040 - crf_accuracy: 0.9805 - val_loss: 10.9787 - val_crf_accuracy: 0.9540
Epoch 00018: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep18.h5
Epoch 19/200
4948/4948 [==============================] - 768s 155ms/step - loss: 10.6057 - crf_accuracy: 0.9796 - val_loss: 10.9751 - val_crf_accuracy: 0.9510
Epoch 00019: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep19.h5
Epoch 20/200
4948/4948 [==============================] - 773s 156ms/step - loss: 10.6072 - crf_accuracy: 0.9781 - val_loss: 10.9741 - val_crf_accuracy: 0.9532
Epoch 00020: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep20.h5
Epoch 21/200
4948/4948 [==============================] - 786s 159ms/step - loss: 10.5927 - crf_accuracy: 0.9817 - val_loss: 10.9997 - val_crf_accuracy: 0.9535
Epoch 00021: saving model to ./models/yidu_chinese_roberta_wwm_ext_L-12_H-768_A-12_ep21.h5
Epoch 00021: early stopping