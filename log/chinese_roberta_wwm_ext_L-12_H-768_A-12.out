ssh://bureaux@172.30.2.148:22/home/bureaux/miniconda3/envs/Bert-base/bin/python -u /home/bureaux/Projects/PulmonaryNodule/train.py
Using TensorFlow backend.
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
结合"1、左肺上叶尖后段结节影较前稍大、密实，建议随访或进一步检查除外占位；余左肺多发结节影部分较前缩小，请随访；2.左肺上叶上舌段支气管扩张合并感染可能较前相仿；3、右肺结节影及斑片影较前相仿，炎症可能，真菌性感染待排；4、纵隔内多发淋巴结较前相仿。 ['O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-ORGAN', 'I-ORGAN', 'I-ORGAN', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-SIGN', 'I-SIGN', 'B-QUANTITY', 'I-QUANTITY', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O']
对比2019-05-23：两肺纹理增深，左肺上叶尖后段结节影较前稍大、密实，周围表现大致相仿；余左肺多发大小不等结节影部分较前缩小；左肺下叶背段结节空洞较前不明显；左肺上叶上舌段纵隔旁局部多发囊状透亮影较前相仿，右肺上叶前段及下叶背段见结节影及斑片状模糊影较前大致相仿。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'O', 'B-TEXTURE', 'I-TEXTURE', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
纵隔心影基本居中，心影大小可。 ['B-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
主动脉弓旁及上腔静脉后淋巴结多发淋巴结较前相仿。 ['B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'B-ORGAN', 'I-ORGAN', 'I-ORGAN', 'B-QUANTITY', 'I-QUANTITY', 'B-ORGAN', 'I-ORGAN', 'I-ORGAN', 'O', 'O', 'O', 'O', 'O']
纵隔肺门血管界面显示欠清。 ['B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-ORGAN', 'I-ORGAN', 'O', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
"两肺散在微小结节影，主动脉弓右前方囊性灶。 ['O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O']
附见肝脏脂肪浸润，副脾。 ['O', 'O', 'B-ORGAN', 'I-ORGAN', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'O']
Img62、71、97示右侧水平裂处、右中肺及左下肺结节影两肺门区未见异常密度影。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
胸膜未见增厚改变。 ['B-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
主动脉，肺动脉主干及其左右分支内造影剂密度均匀。 ['B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'B-DENSITY', 'I-DENSITY', 'O']
"左下肺局限性肺不张，纵隔左移，结核待排两上肺散在炎症，较前2020-5-19吸收左肺结节，较前相仿左肺下叶肺大泡；左侧胸廓较对侧略塌陷，左下肺不张，两上肺散在斑片影，左肺多发小结节，良性增殖灶可能，左肺下叶肺大泡。 ['O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'O', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'O', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-NATURE', 'I-NATURE', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O']
气管及双侧支气管通畅。 ['B-ORGAN', 'I-ORGAN', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-SIGN', 'I-SIGN', 'O']
纵隔、肺门未见明显肿大淋巴结影。 ['B-ANATOMY', 'I-ANATOMY', 'O', 'B-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
左侧胸腔少量积液。 ['B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'O']
两侧胸膜未见明显增厚。 ['B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O']
""左肺术后改变同前，请随访。 ['O', 'O', 'B-TREATMENT', 'I-TREATMENT', 'I-TREATMENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
右肺结节治疗后，右肺上叶磨玻璃结节及实性结节，大致同前，建议结合临床随访。 ['B-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'B-DISEASE', 'I-DISEASE', 'I-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
右肺数枚微小结节同前，良性，请随访。 ['B-ANATOMY', 'I-ANATOMY', 'B-QUANTITY', 'I-QUANTITY', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'I-SIGN', 'O', 'O', 'O', 'B-NATURE', 'I-NATURE', 'O', 'O', 'O', 'O', 'O']
右肺上叶炎症同前相仿，良性可能，建议随访。 ['B-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'I-ANATOMY', 'B-DISEASE', 'I-DISEASE', 'O', 'O', 'O', 'O', 'O', 'B-NATURE', 'I-NATURE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
对比2019-12-3片：左肺术后，左肺见条索影。 ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TREATMENT', 'I-TREATMENT', 'I-TREATMENT', 'O', 'O', 'B-ANATOMY', 'I-ANATOMY', 'O', 'B-SIGN', 'I-SIGN', 'I-SIGN', 'O']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

load bert Model start!
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.

load bert Model end!
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input-Token (InputLayer)        (None, 150)          0                                            
__________________________________________________________________________________________________
Input-Segment (InputLayer)      (None, 150)          0                                            
__________________________________________________________________________________________________
Embedding-Token (TokenEmbedding [(None, 150, 768), ( 16226304    Input-Token[0][0]                
__________________________________________________________________________________________________
Embedding-Segment (Embedding)   (None, 150, 768)     1536        Input-Segment[0][0]              
__________________________________________________________________________________________________
Embedding-Token-Segment (Add)   (None, 150, 768)     0           Embedding-Token[0][0]            
                                                                 Embedding-Segment[0][0]          
__________________________________________________________________________________________________
Embedding-Position (PositionEmb (None, 150, 768)     115200      Embedding-Token-Segment[0][0]    
__________________________________________________________________________________________________
Embedding-Dropout (Dropout)     (None, 150, 768)     0           Embedding-Position[0][0]         
__________________________________________________________________________________________________
Embedding-Norm (LayerNormalizat (None, 150, 768)     1536        Embedding-Dropout[0][0]          
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 150, 768)     2362368     Embedding-Norm[0][0]             
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-1-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 150, 768)     0           Embedding-Norm[0][0]             
                                                                 Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-1-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-1-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-1-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-1-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-1-MultiHeadSelfAttention-
                                                                 Encoder-1-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-1-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-1-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-2-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-1-FeedForward-Norm[0][0] 
                                                                 Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-2-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-2-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-2-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-2-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-2-MultiHeadSelfAttention-
                                                                 Encoder-2-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-2-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-2-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-3-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-2-FeedForward-Norm[0][0] 
                                                                 Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-3-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-3-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-3-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-3-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-3-MultiHeadSelfAttention-
                                                                 Encoder-3-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-3-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-3-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-4-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-3-FeedForward-Norm[0][0] 
                                                                 Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-4-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-4-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-4-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-4-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-4-MultiHeadSelfAttention-
                                                                 Encoder-4-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-4-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-4-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-5-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-4-FeedForward-Norm[0][0] 
                                                                 Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-5-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-5-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-5-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-5-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-5-MultiHeadSelfAttention-
                                                                 Encoder-5-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-5-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-5-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-6-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-5-FeedForward-Norm[0][0] 
                                                                 Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-6-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-6-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-6-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-6-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-6-MultiHeadSelfAttention-
                                                                 Encoder-6-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-6-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-6-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-7-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-6-FeedForward-Norm[0][0] 
                                                                 Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-7-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-7-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-7-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-7-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-7-MultiHeadSelfAttention-
                                                                 Encoder-7-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-7-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-7-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-8-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-7-FeedForward-Norm[0][0] 
                                                                 Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-8-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-8-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-8-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-8-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-8-MultiHeadSelfAttention-
                                                                 Encoder-8-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-8-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-8-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 150, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-9-MultiHeadSelfAttention[
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 150, 768)     0           Encoder-8-FeedForward-Norm[0][0] 
                                                                 Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-MultiHeadSelfAttentio (None, 150, 768)     1536        Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-FeedForward (FeedForw (None, 150, 768)     4722432     Encoder-9-MultiHeadSelfAttention-
__________________________________________________________________________________________________
Encoder-9-FeedForward-Dropout ( (None, 150, 768)     0           Encoder-9-FeedForward[0][0]      
__________________________________________________________________________________________________
Encoder-9-FeedForward-Add (Add) (None, 150, 768)     0           Encoder-9-MultiHeadSelfAttention-
                                                                 Encoder-9-FeedForward-Dropout[0][
__________________________________________________________________________________________________
Encoder-9-FeedForward-Norm (Lay (None, 150, 768)     1536        Encoder-9-FeedForward-Add[0][0]  
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 150, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] 
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-9-FeedForward-Norm[0][0] 
                                                                 Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-MultiHeadSelfAttenti (None, 150, 768)     1536        Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-FeedForward (FeedFor (None, 150, 768)     4722432     Encoder-10-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-10-FeedForward-Dropout  (None, 150, 768)     0           Encoder-10-FeedForward[0][0]     
__________________________________________________________________________________________________
Encoder-10-FeedForward-Add (Add (None, 150, 768)     0           Encoder-10-MultiHeadSelfAttention
                                                                 Encoder-10-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-10-FeedForward-Norm (La (None, 150, 768)     1536        Encoder-10-FeedForward-Add[0][0] 
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 150, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-10-FeedForward-Norm[0][0]
                                                                 Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-MultiHeadSelfAttenti (None, 150, 768)     1536        Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-FeedForward (FeedFor (None, 150, 768)     4722432     Encoder-11-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-11-FeedForward-Dropout  (None, 150, 768)     0           Encoder-11-FeedForward[0][0]     
__________________________________________________________________________________________________
Encoder-11-FeedForward-Add (Add (None, 150, 768)     0           Encoder-11-MultiHeadSelfAttention
                                                                 Encoder-11-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-11-FeedForward-Norm (La (None, 150, 768)     1536        Encoder-11-FeedForward-Add[0][0] 
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 150, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 150, 768)     0           Encoder-11-FeedForward-Norm[0][0]
                                                                 Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-MultiHeadSelfAttenti (None, 150, 768)     1536        Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-FeedForward (FeedFor (None, 150, 768)     4722432     Encoder-12-MultiHeadSelfAttention
__________________________________________________________________________________________________
Encoder-12-FeedForward-Dropout  (None, 150, 768)     0           Encoder-12-FeedForward[0][0]     
__________________________________________________________________________________________________
Encoder-12-FeedForward-Add (Add (None, 150, 768)     0           Encoder-12-MultiHeadSelfAttention
                                                                 Encoder-12-FeedForward-Dropout[0]
__________________________________________________________________________________________________
Encoder-12-FeedForward-Norm (La (None, 150, 768)     1536        Encoder-12-FeedForward-Add[0][0] 
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 150, 256)     918528      Encoder-12-FeedForward-Norm[0][0]
__________________________________________________________________________________________________
time_distributed_1 (TimeDistrib (None, 150, 33)      8481        bidirectional_1[0][0]            
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 150, 33)      0           time_distributed_1[0][0]         
__________________________________________________________________________________________________
crf_1 (CRF)                     (None, 150, 33)      2277        dropout_1[0][0]                  
==================================================================================================
Total params: 102,328,326
Trainable params: 102,328,326
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:From /home/bureaux/miniconda3/envs/Bert-base/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

Epoch 1/500
807/807 [==============================] - 66s 82ms/step - loss: 28.7163 - crf_accuracy: 0.0347

Epoch 00001: loss improved from inf to 28.71630, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 2/500
807/807 [==============================] - 45s 55ms/step - loss: 28.5250 - crf_accuracy: 0.1092

Epoch 00002: loss improved from 28.71630 to 28.52504, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 3/500
807/807 [==============================] - 42s 52ms/step - loss: 28.1740 - crf_accuracy: 0.3053

Epoch 00003: loss improved from 28.52504 to 28.17403, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 4/500
807/807 [==============================] - 45s 56ms/step - loss: 27.8161 - crf_accuracy: 0.3754

Epoch 00004: loss improved from 28.17403 to 27.81609, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 5/500
807/807 [==============================] - 45s 56ms/step - loss: 27.5356 - crf_accuracy: 0.3995

Epoch 00005: loss improved from 27.81609 to 27.53562, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 6/500
807/807 [==============================] - 45s 56ms/step - loss: 27.2555 - crf_accuracy: 0.4752

Epoch 00006: loss improved from 27.53562 to 27.25547, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 7/500
807/807 [==============================] - 45s 56ms/step - loss: 26.9431 - crf_accuracy: 0.5800

Epoch 00007: loss improved from 27.25547 to 26.94308, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 8/500
807/807 [==============================] - 45s 56ms/step - loss: 26.6507 - crf_accuracy: 0.6564

Epoch 00008: loss improved from 26.94308 to 26.65066, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 9/500
807/807 [==============================] - 43s 53ms/step - loss: 26.4221 - crf_accuracy: 0.7019

Epoch 00009: loss improved from 26.65066 to 26.42211, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 10/500
807/807 [==============================] - 44s 54ms/step - loss: 26.2419 - crf_accuracy: 0.7582

Epoch 00010: loss improved from 26.42211 to 26.24194, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 11/500
807/807 [==============================] - 44s 54ms/step - loss: 26.1017 - crf_accuracy: 0.7943

Epoch 00011: loss improved from 26.24194 to 26.10171, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 12/500
807/807 [==============================] - 43s 53ms/step - loss: 25.9824 - crf_accuracy: 0.8246

Epoch 00012: loss improved from 26.10171 to 25.98236, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 13/500
807/807 [==============================] - 46s 57ms/step - loss: 25.9014 - crf_accuracy: 0.8399

Epoch 00013: loss improved from 25.98236 to 25.90143, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 14/500
807/807 [==============================] - 45s 55ms/step - loss: 25.8255 - crf_accuracy: 0.8568

Epoch 00014: loss improved from 25.90143 to 25.82547, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 15/500
807/807 [==============================] - 45s 56ms/step - loss: 25.7693 - crf_accuracy: 0.8634

Epoch 00015: loss improved from 25.82547 to 25.76934, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 16/500
807/807 [==============================] - 45s 55ms/step - loss: 25.6953 - crf_accuracy: 0.8779

Epoch 00016: loss improved from 25.76934 to 25.69526, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 17/500
807/807 [==============================] - 44s 55ms/step - loss: 25.6451 - crf_accuracy: 0.8872

Epoch 00017: loss improved from 25.69526 to 25.64505, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 18/500
807/807 [==============================] - 45s 56ms/step - loss: 25.5926 - crf_accuracy: 0.8948

Epoch 00018: loss improved from 25.64505 to 25.59256, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 19/500
807/807 [==============================] - 45s 56ms/step - loss: 25.5518 - crf_accuracy: 0.9023

Epoch 00019: loss improved from 25.59256 to 25.55180, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 20/500
807/807 [==============================] - 45s 56ms/step - loss: 25.5211 - crf_accuracy: 0.9108

Epoch 00020: loss improved from 25.55180 to 25.52109, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 21/500
807/807 [==============================] - 45s 55ms/step - loss: 25.4786 - crf_accuracy: 0.9166

Epoch 00021: loss improved from 25.52109 to 25.47861, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 22/500
807/807 [==============================] - 44s 55ms/step - loss: 25.4605 - crf_accuracy: 0.9218

Epoch 00022: loss improved from 25.47861 to 25.46049, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 23/500
807/807 [==============================] - 45s 55ms/step - loss: 25.4302 - crf_accuracy: 0.9293

Epoch 00023: loss improved from 25.46049 to 25.43016, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 24/500
807/807 [==============================] - 45s 56ms/step - loss: 25.3982 - crf_accuracy: 0.9329

Epoch 00024: loss improved from 25.43016 to 25.39821, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 25/500
807/807 [==============================] - 44s 55ms/step - loss: 25.3742 - crf_accuracy: 0.9419

Epoch 00025: loss improved from 25.39821 to 25.37421, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 26/500
807/807 [==============================] - 45s 55ms/step - loss: 25.3571 - crf_accuracy: 0.9456

Epoch 00026: loss improved from 25.37421 to 25.35707, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 27/500
807/807 [==============================] - 44s 55ms/step - loss: 25.3300 - crf_accuracy: 0.9515

Epoch 00027: loss improved from 25.35707 to 25.33001, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 28/500
807/807 [==============================] - 44s 55ms/step - loss: 25.3169 - crf_accuracy: 0.9542

Epoch 00028: loss improved from 25.33001 to 25.31689, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 29/500
807/807 [==============================] - 44s 55ms/step - loss: 25.3174 - crf_accuracy: 0.9539

Epoch 00029: loss did not improve from 25.31689
Epoch 30/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2875 - crf_accuracy: 0.9608

Epoch 00030: loss improved from 25.31689 to 25.28753, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 31/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2795 - crf_accuracy: 0.9638

Epoch 00031: loss improved from 25.28753 to 25.27951, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 32/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2668 - crf_accuracy: 0.9660

Epoch 00032: loss improved from 25.27951 to 25.26682, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 33/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2597 - crf_accuracy: 0.9678

Epoch 00033: loss improved from 25.26682 to 25.25972, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 34/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2582 - crf_accuracy: 0.9686

Epoch 00034: loss improved from 25.25972 to 25.25822, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 35/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2567 - crf_accuracy: 0.9694

Epoch 00035: loss improved from 25.25822 to 25.25669, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 36/500
807/807 [==============================] - 44s 54ms/step - loss: 25.2599 - crf_accuracy: 0.9684

Epoch 00036: loss did not improve from 25.25669
Epoch 37/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2474 - crf_accuracy: 0.9697

Epoch 00037: loss improved from 25.25669 to 25.24741, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 38/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2477 - crf_accuracy: 0.9707

Epoch 00038: loss did not improve from 25.24741
Epoch 39/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2336 - crf_accuracy: 0.9738

Epoch 00039: loss improved from 25.24741 to 25.23360, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 40/500
807/807 [==============================] - 46s 57ms/step - loss: 25.2167 - crf_accuracy: 0.9765

Epoch 00040: loss improved from 25.23360 to 25.21675, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 41/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2078 - crf_accuracy: 0.9791

Epoch 00041: loss improved from 25.21675 to 25.20778, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 42/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2021 - crf_accuracy: 0.9796

Epoch 00042: loss improved from 25.20778 to 25.20209, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 43/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2028 - crf_accuracy: 0.9808

Epoch 00043: loss did not improve from 25.20209
Epoch 44/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2118 - crf_accuracy: 0.9782

Epoch 00044: loss did not improve from 25.20209
Epoch 45/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1957 - crf_accuracy: 0.9809

Epoch 00045: loss improved from 25.20209 to 25.19566, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 46/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1996 - crf_accuracy: 0.9801

Epoch 00046: loss did not improve from 25.19566
Epoch 47/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1997 - crf_accuracy: 0.9830

Epoch 00047: loss did not improve from 25.19566
Epoch 48/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1893 - crf_accuracy: 0.9836

Epoch 00048: loss improved from 25.19566 to 25.18927, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 49/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2047 - crf_accuracy: 0.9794

Epoch 00049: loss did not improve from 25.18927
Epoch 50/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2021 - crf_accuracy: 0.9802

Epoch 00050: loss did not improve from 25.18927
Epoch 51/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1970 - crf_accuracy: 0.9809

Epoch 00051: loss did not improve from 25.18927
Epoch 52/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1873 - crf_accuracy: 0.9832

Epoch 00052: loss improved from 25.18927 to 25.18733, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 53/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1923 - crf_accuracy: 0.9831

Epoch 00053: loss did not improve from 25.18733
Epoch 54/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1985 - crf_accuracy: 0.9809

Epoch 00054: loss did not improve from 25.18733
Epoch 55/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2309 - crf_accuracy: 0.9736

Epoch 00055: loss did not improve from 25.18733
Epoch 56/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2208 - crf_accuracy: 0.9746

Epoch 00056: loss did not improve from 25.18733
Epoch 57/500
807/807 [==============================] - 44s 55ms/step - loss: 25.2006 - crf_accuracy: 0.9795

Epoch 00057: loss did not improve from 25.18733
Epoch 58/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1974 - crf_accuracy: 0.9823

Epoch 00058: loss did not improve from 25.18733
Epoch 59/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1925 - crf_accuracy: 0.9832

Epoch 00059: loss did not improve from 25.18733
Epoch 60/500
807/807 [==============================] - 45s 55ms/step - loss: 25.2051 - crf_accuracy: 0.9807

Epoch 00060: loss did not improve from 25.18733
Epoch 61/500
807/807 [==============================] - 45s 56ms/step - loss: 25.2142 - crf_accuracy: 0.9773

Epoch 00061: loss did not improve from 25.18733
Epoch 62/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1832 - crf_accuracy: 0.9836

Epoch 00062: loss improved from 25.18733 to 25.18320, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 63/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1756 - crf_accuracy: 0.9855

Epoch 00063: loss improved from 25.18320 to 25.17556, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 64/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1689 - crf_accuracy: 0.9864

Epoch 00064: loss improved from 25.17556 to 25.16889, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 65/500
807/807 [==============================] - 46s 57ms/step - loss: 25.1589 - crf_accuracy: 0.9888

Epoch 00065: loss improved from 25.16889 to 25.15892, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 66/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1556 - crf_accuracy: 0.9887

Epoch 00066: loss improved from 25.15892 to 25.15556, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 67/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1548 - crf_accuracy: 0.9896

Epoch 00067: loss improved from 25.15556 to 25.15475, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 68/500
807/807 [==============================] - 43s 54ms/step - loss: 25.1548 - crf_accuracy: 0.9895

Epoch 00068: loss did not improve from 25.15475
Epoch 69/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1559 - crf_accuracy: 0.9885

Epoch 00069: loss did not improve from 25.15475
Epoch 70/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1507 - crf_accuracy: 0.9896

Epoch 00070: loss improved from 25.15475 to 25.15071, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 71/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1483 - crf_accuracy: 0.9902

Epoch 00071: loss improved from 25.15071 to 25.14829, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 72/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1463 - crf_accuracy: 0.9906

Epoch 00072: loss improved from 25.14829 to 25.14626, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 73/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1432 - crf_accuracy: 0.9917

Epoch 00073: loss improved from 25.14626 to 25.14318, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 74/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1410 - crf_accuracy: 0.9920

Epoch 00074: loss improved from 25.14318 to 25.14096, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 75/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1461 - crf_accuracy: 0.9908

Epoch 00075: loss did not improve from 25.14096
Epoch 76/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1457 - crf_accuracy: 0.9902

Epoch 00076: loss did not improve from 25.14096
Epoch 77/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1456 - crf_accuracy: 0.9907

Epoch 00077: loss did not improve from 25.14096
Epoch 78/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1435 - crf_accuracy: 0.9909

Epoch 00078: loss did not improve from 25.14096
Epoch 79/500
807/807 [==============================] - 40s 49ms/step - loss: 25.1416 - crf_accuracy: 0.9915

Epoch 00079: loss did not improve from 25.14096
Epoch 80/500
807/807 [==============================] - 43s 53ms/step - loss: 25.1398 - crf_accuracy: 0.9914

Epoch 00080: loss improved from 25.14096 to 25.13983, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 81/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1374 - crf_accuracy: 0.9918

Epoch 00081: loss improved from 25.13983 to 25.13739, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 82/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1382 - crf_accuracy: 0.9916

Epoch 00082: loss did not improve from 25.13739
Epoch 83/500
807/807 [==============================] - 45s 55ms/step - loss: 25.1385 - crf_accuracy: 0.9920

Epoch 00083: loss did not improve from 25.13739
Epoch 84/500
807/807 [==============================] - 43s 54ms/step - loss: 25.1353 - crf_accuracy: 0.9924

Epoch 00084: loss improved from 25.13739 to 25.13526, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 85/500
807/807 [==============================] - 44s 54ms/step - loss: 25.1361 - crf_accuracy: 0.9918

Epoch 00085: loss did not improve from 25.13526
Epoch 86/500
807/807 [==============================] - 43s 54ms/step - loss: 25.1350 - crf_accuracy: 0.9928

Epoch 00086: loss improved from 25.13526 to 25.13503, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 87/500
807/807 [==============================] - 46s 57ms/step - loss: 25.1354 - crf_accuracy: 0.9923

Epoch 00087: loss did not improve from 25.13503
Epoch 88/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1345 - crf_accuracy: 0.9926

Epoch 00088: loss improved from 25.13503 to 25.13451, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 89/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1342 - crf_accuracy: 0.9926

Epoch 00089: loss improved from 25.13451 to 25.13416, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 90/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1354 - crf_accuracy: 0.9931

Epoch 00090: loss did not improve from 25.13416
Epoch 91/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1335 - crf_accuracy: 0.9923

Epoch 00091: loss improved from 25.13416 to 25.13355, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 92/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1339 - crf_accuracy: 0.9922

Epoch 00092: loss did not improve from 25.13355
Epoch 93/500
807/807 [==============================] - 44s 55ms/step - loss: 25.1357 - crf_accuracy: 0.9918

Epoch 00093: loss did not improve from 25.13355
Epoch 94/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1326 - crf_accuracy: 0.9923

Epoch 00094: loss improved from 25.13355 to 25.13257, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 95/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1338 - crf_accuracy: 0.9927

Epoch 00095: loss did not improve from 25.13257
Epoch 96/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1319 - crf_accuracy: 0.9921

Epoch 00096: loss improved from 25.13257 to 25.13186, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 97/500
807/807 [==============================] - 46s 57ms/step - loss: 25.1316 - crf_accuracy: 0.9928

Epoch 00097: loss improved from 25.13186 to 25.13161, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 98/500
807/807 [==============================] - 45s 56ms/step - loss: 25.1331 - crf_accuracy: 0.9916

Epoch 00098: loss did not improve from 25.13161
Epoch 99/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1326 - crf_accuracy: 0.9923

Epoch 00099: loss did not improve from 25.13161
Epoch 100/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1336 - crf_accuracy: 0.9919

Epoch 00100: loss did not improve from 25.13161
Epoch 101/500
807/807 [==============================] - 43s 53ms/step - loss: 25.1356 - crf_accuracy: 0.9917

Epoch 00101: loss did not improve from 25.13161
Epoch 102/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1325 - crf_accuracy: 0.9928

Epoch 00102: loss did not improve from 25.13161
Epoch 103/500
807/807 [==============================] - 40s 50ms/step - loss: 25.1302 - crf_accuracy: 0.9924

Epoch 00103: loss improved from 25.13161 to 25.13016, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 104/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1304 - crf_accuracy: 0.9923

Epoch 00104: loss did not improve from 25.13016
Epoch 105/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1316 - crf_accuracy: 0.9922

Epoch 00105: loss did not improve from 25.13016
Epoch 106/500
807/807 [==============================] - 40s 50ms/step - loss: 25.1305 - crf_accuracy: 0.9923

Epoch 00106: loss did not improve from 25.13016
Epoch 107/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1293 - crf_accuracy: 0.9926

Epoch 00107: loss improved from 25.13016 to 25.12926, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 108/500
807/807 [==============================] - 40s 50ms/step - loss: 25.1315 - crf_accuracy: 0.9927

Epoch 00108: loss did not improve from 25.12926
Epoch 109/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1299 - crf_accuracy: 0.9926

Epoch 00109: loss did not improve from 25.12926
Epoch 110/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1298 - crf_accuracy: 0.9929

Epoch 00110: loss did not improve from 25.12926
Epoch 111/500
807/807 [==============================] - 41s 50ms/step - loss: 25.1284 - crf_accuracy: 0.9927

Epoch 00111: loss improved from 25.12926 to 25.12845, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 112/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1298 - crf_accuracy: 0.9927

Epoch 00112: loss did not improve from 25.12845
Epoch 113/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1318 - crf_accuracy: 0.9924

Epoch 00113: loss did not improve from 25.12845
Epoch 114/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1276 - crf_accuracy: 0.9928

Epoch 00114: loss improved from 25.12845 to 25.12764, saving model to ./models/pulmonary_chinese_roberta_wwm_ext_L-12_H-768_A-12_ner.h5
Epoch 115/500
807/807 [==============================] - 41s 50ms/step - loss: 25.1304 - crf_accuracy: 0.9927

Epoch 00115: loss did not improve from 25.12764
Epoch 116/500
807/807 [==============================] - 34s 42ms/step - loss: 25.1295 - crf_accuracy: 0.9929

Epoch 00116: loss did not improve from 25.12764
Epoch 117/500
807/807 [==============================] - 41s 50ms/step - loss: 25.1293 - crf_accuracy: 0.9924

Epoch 00117: loss did not improve from 25.12764
Epoch 118/500
807/807 [==============================] - 41s 50ms/step - loss: 25.1281 - crf_accuracy: 0.9927

Epoch 00118: loss did not improve from 25.12764
Epoch 119/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1311 - crf_accuracy: 0.9921

Epoch 00119: loss did not improve from 25.12764
Epoch 120/500
807/807 [==============================] - 41s 51ms/step - loss: 25.1281 - crf_accuracy: 0.9932

Epoch 00120: loss did not improve from 25.12764
Epoch 121/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1311 - crf_accuracy: 0.9927

Epoch 00121: loss did not improve from 25.12764
Epoch 122/500
807/807 [==============================] - 40s 50ms/step - loss: 25.1309 - crf_accuracy: 0.9924

Epoch 00122: loss did not improve from 25.12764
Epoch 123/500
807/807 [==============================] - 40s 50ms/step - loss: 25.1294 - crf_accuracy: 0.9923

Epoch 00123: loss did not improve from 25.12764
Epoch 124/500
807/807 [==============================] - 42s 52ms/step - loss: 25.1306 - crf_accuracy: 0.9920

Epoch 00124: loss did not improve from 25.12764
Epoch 00124: early stopping

进程已结束,退出代码0
